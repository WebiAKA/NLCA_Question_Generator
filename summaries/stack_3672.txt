The author describes how structuring data for classification is a challenge because the accuracy of the model depends on training and testing data, but the same model trained on only some of the data is different than when it was tested on all of the data. The author also worries that if a classifier were trained and tested with small subsets of data they would learn to overfit and under-fit future measurements. The author points out that validation sets are an essential part of optimally designing training sets. A machine learning algorithm's accuracy can change dramatically depending on whether training or testing data is used as input. A classification method's accuracy will be different if it's trained only on experiments 1-80 and tested on 81-100 versus mixing all the data from all shots and randomly choosing 80% for my training set and the remaining 20% for my test set. In this case, there is no general consensus on which approach to take because each has a positive and negative outcome (the first scenario